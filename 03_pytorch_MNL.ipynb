{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp pytorch_MNL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNL with Pytorch\n",
    "\n",
    "> API details of MNL with Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MNL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import math\n",
    "# serialization and deserialization of model\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class MNL(nn.Module):\n",
    "    '''\n",
    "        The Multinomial Logistic Regression model implemented with Pytorch\n",
    "    '''\n",
    "    def __init__(self, feature_list):\n",
    "        super(MNL, self).__init__()\n",
    "        \n",
    "        self.feature_list = feature_list\n",
    "        input_dim = len(feature_list)\n",
    "        # a linear layer without bias\n",
    "        self.linear = torch.nn.Linear(input_dim, 1, bias=False)\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # expect the input to be a session of alternatives\n",
    "        util_values = self.linear(x)\n",
    "        \n",
    "        #!! a trik to prevent the underflow (divide by zero) in the softmax later\n",
    "        util_values = util_values + 2\n",
    "        \n",
    "        # transpose the result vector before the softmax \n",
    "        util_values = torch.t(util_values)\n",
    "        \n",
    "        # convert the softmax values to binary values\n",
    "        #max_values, indices = self.softmax(util_values).max()\n",
    "    \n",
    "        #results = np.zeros(len(x))\n",
    "        #results[indices] = 1\n",
    "        #results = np.transpose(results)\n",
    "        \n",
    "        return torch.t(self.softmax(util_values))\n",
    "\n",
    "    \n",
    "    def l1_loss(self, l1_weight=0.01):\n",
    "        '''\n",
    "            Return: L1 regularization on all the parameters\n",
    "        '''\n",
    "        params_list = []\n",
    "        for param in self.parameters():\n",
    "            params_list.append(param.view(-1))\n",
    "        torch_params = torch.cat(params_list)\n",
    "            \n",
    "        return l1_weight * (torch.abs(torch_params).sum())\n",
    "\n",
    "\n",
    "    def l2_loss(self, l2_weight=0.01):\n",
    "        '''\n",
    "            Return: L2 regularization on all the parameters\n",
    "        '''\n",
    "        params_list = []    \n",
    "        for param in self.parameters():\n",
    "            params_list.append(param.view(-1))\n",
    "        torch_params = torch.cat(params_list)\n",
    "            \n",
    "        return l2_weight * (torch.sqrt(torch.pow(torch_params, 2).sum()))\n",
    "\n",
    "\n",
    "    def train(self, loss, optimizer, x_val, y_val,\n",
    "              l1_loss_weight = 0,  # when zero, no L1 regularization\n",
    "              l2_loss_weight = 0,\n",
    "              gpu=False):\n",
    "        \"\"\"\n",
    "            Train the model with a batch (in our case, also a session) of data\n",
    "        \"\"\"\n",
    "        # expect y_val to be of one_dimension\n",
    "        y_val = y_val.reshape(len(y_val), 1)\n",
    "\n",
    "        tensorX = torch.from_numpy(x_val).double()\n",
    "        tensorY = torch.from_numpy(y_val).double()\n",
    "\n",
    "        if (gpu):\n",
    "            dtype = torch.cuda.DoubleTensor\n",
    "        else:\n",
    "            dtype = torch.DoubleTensor\n",
    "\n",
    "        # input variable\n",
    "        x = Variable(tensorX.type(dtype), requires_grad=False)\n",
    "        # target variable\n",
    "        y = Variable(tensorY.type(dtype), requires_grad=False)\n",
    "\n",
    "        # Forward to calculate the losses\n",
    "        fx = self.forward(x)\n",
    "        data_loss = loss.forward(fx, y)\n",
    "\n",
    "        # optional: add L1 or L2 penalities for regularization\n",
    "        if (l1_loss_weight != 0):\n",
    "            l1_loss = self.l1_loss(l1_loss_weight)\n",
    "            output = data_loss + l1_loss\n",
    "\n",
    "        elif (l2_loss_weight != 0):\n",
    "            l2_loss = self.l2_loss(l2_loss_weight)\n",
    "            output = data_loss + l2_loss\n",
    "\n",
    "        else:\n",
    "            output = data_loss\n",
    "\n",
    "        # Underflow in the loss calculation\n",
    "        if math.isnan(output.data[0]):\n",
    "            raise ValueError('NaN detected')\n",
    "            #return output.data[0]\n",
    "\n",
    "        if (isinstance(optimizer, torch.optim.LBFGS)):\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                output.backward(retain_graph=True)\n",
    "                return output\n",
    "\n",
    "            optimizer.step(closure)\n",
    "        else:\n",
    "            # Reset gradient\n",
    "            optimizer.zero_grad()\n",
    "            # Backward\n",
    "            output.backward()\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "        # return the cost\n",
    "        return output.data[0]\n",
    "\n",
    "\n",
    "    def predict(self, x_val, binary=False):\n",
    "        '''\n",
    "            Give prediction for alternatives within a single session\n",
    "            x_val: DataFrame, or np.ndarray\n",
    "            return: numpy\n",
    "        '''\n",
    "        is_gpu = self.get_params()[0].is_cuda\n",
    "\n",
    "        if isinstance(x_val, pd.DataFrame):\n",
    "            tensorX = torch.from_numpy(x_val.values).double()\n",
    "        else:\n",
    "            tensorX = torch.from_numpy(x_val).double()\n",
    "    \n",
    "        if (is_gpu):\n",
    "            x = Variable(tensorX.type(torch.cuda.DoubleTensor), requires_grad=False)\n",
    "        else:\n",
    "            x = Variable(tensorX, requires_grad=False)\n",
    "\n",
    "        output = self.forward(x)\n",
    "    \n",
    "        if (is_gpu):\n",
    "            # get the data from the memory of GPU into CPU\n",
    "            prob = output.cpu().data.numpy()\n",
    "        else:\n",
    "            prob = output.data.numpy()\n",
    "        \n",
    "        if (binary):\n",
    "            # convert the softmax values to binary values\n",
    "            max_indice = prob.argmax(axis=0)\n",
    "            ret = np.zeros(len(prob))\n",
    "            ret[max_indice] = 1\n",
    "            return ret\n",
    "        else:\n",
    "            return prob\n",
    "\n",
    "\n",
    "    def get_params(self):\n",
    "        '''\n",
    "            Return the Variable of the MNL parameters,\n",
    "              which can be updated manually.\n",
    "        '''\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.requires_grad and name == 'linear.weight':\n",
    "                return param\n",
    "        return None\n",
    "\n",
    "    \n",
    "    def print_params(self):\n",
    "        '''\n",
    "            Print all the parameters within the model\n",
    "        '''\n",
    "        params = self.get_params()[0]\n",
    "        \n",
    "        if (params.is_cuda):\n",
    "            values = params.cpu().data.numpy()\n",
    "        else:\n",
    "            values = params.data.numpy()\n",
    "        \n",
    "        for index, feature in enumerate(self.feature_list):\n",
    "            print(feature, ':', values[index])\n",
    "    \n",
    "    \n",
    "    def get_feature_weight(self, feature_name):\n",
    "        ''' Retrieve the weight of the desired feature '''\n",
    "        params = self.get_params()[0]\n",
    "\n",
    "        if (params.is_cuda):\n",
    "            param_values = params.cpu().data.numpy()\n",
    "        else:\n",
    "            param_values = params.data.numpy()\n",
    "        \n",
    "        for index, feature in enumerate(self.feature_list):\n",
    "            if (feature_name == feature):\n",
    "                return param_values[index]\n",
    "        \n",
    "        # did not find the specified feature\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def get_feature_weights(self):\n",
    "        ''' get the dictionary of feature weights '''\n",
    "        params = self.get_params()[0]\n",
    "\n",
    "        if (params.is_cuda):\n",
    "            param_values = params.cpu().data.numpy()\n",
    "        else:\n",
    "            param_values = params.data.numpy()\n",
    "        \n",
    "        feature_weights = {}\n",
    "        \n",
    "        for index, feature in enumerate(self.feature_list):\n",
    "            feature_weights[feature] = param_values[index]\n",
    "        \n",
    "        return feature_weights\n",
    "    \n",
    "    \n",
    "    def set_feature_weight(self, feature_name, value):\n",
    "        ''' Reset the specified feature weight\n",
    "        '''\n",
    "        params = self.get_params()[0]\n",
    "        is_found = False\n",
    "        \n",
    "        try:\n",
    "            for index, feature in enumerate(self.feature_list):\n",
    "                if (feature_name == feature):\n",
    "                    is_found = True\n",
    "                    # override the parameters within the model\n",
    "                    params[index] = value\n",
    "        except RuntimeError as e:\n",
    "            #print('RuntimeError: ', e)\n",
    "            #print('One can ignore this error, since the parameters are still updated !')\n",
    "            pass\n",
    "        \n",
    "        return is_found\n",
    "\n",
    "\n",
    "    def save(self, file_name):\n",
    "        '''\n",
    "            Serialize the model object into a file\n",
    "        '''\n",
    "        with open(file_name, mode='wb') as model_file:\n",
    "            pickle.dump(self, model_file)\n",
    "            print('save model to ', file_name)\n",
    "\n",
    "\n",
    "    def set_train_config(self, train_config):\n",
    "        '''\n",
    "            Set the training configs along with the model,\n",
    "             so that it can be serialized together with the model.\n",
    "        '''\n",
    "        self.train_config = train_config\n",
    "\n",
    "\n",
    "    def get_train_config(self):\n",
    "        return self.train_config\n",
    "\n",
    "\n",
    "def load_model(pickle_file):\n",
    "    \"\"\"\n",
    "        Instantialize a model from its pickle file.\n",
    "    \"\"\"\n",
    "    with open(pickle_file, 'rb') as inp:\n",
    "        try:\n",
    "            # python 3\n",
    "            model = pickle.load(inp, encoding='bytes')\n",
    "        except:\n",
    "            # python 2\n",
    "            model = pickle.load(inp)\n",
    "\n",
    "        print('load model from ', pickle_file)\n",
    "        return model\n",
    "\n",
    "\n",
    "def build_model(input_dim):\n",
    "    '''\n",
    "        Another way to build the model.\n",
    "    '''\n",
    "    model = torch.nn.Sequential()\n",
    "    model.add_module(\"linear\",\n",
    "                     torch.nn.Linear(input_dim, 1, bias=False))\n",
    "    \n",
    "    # We need the softmax layer here for the binary cross entropy later \n",
    "    model.add_module('softmax', torch.nn.Softmax())\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.6052)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(torch.Tensor([0.01,0.99]), torch.Tensor([1, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
