---

title: pytorch_MNL

keywords: fastai
sidebar: home_sidebar

summary: "API details of pytorch_MNL."
description: "API details of pytorch_MNL."
nb_path: "03_pytorch_MNL.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: 03_pytorch_MNL.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="say_hello" class="doc_header"><code>say_hello</code><a href="https://github.com/danhphan/choicenet/tree/master/choicenet/pytorch_MNL.py#L6" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>say_hello</code>(<strong><code>to</code></strong>)</p>
</blockquote>
<p>Say hello to somebody</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">say_hello</span><span class="p">(</span><span class="s2">&quot;Choice Net&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&#39;Hello Choice Net!&#39;</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">assert</span> <span class="n">say_hello</span><span class="p">(</span><span class="s2">&quot;Choice Net&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;Hello Choice Net!&quot;</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="MNL-model">MNL model<a class="anchor-link" href="#MNL-model"> </a></h4>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>

<span class="kn">import</span> <span class="nn">math</span>
<span class="c1"># serialization and deserialization of model</span>
<span class="kn">import</span> <span class="nn">pickle</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="k">class</span> <span class="nc">MNL</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        The Multinomial Logistic Regression model implemented with Pytorch</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feature_list</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MNL</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">feature_list</span> <span class="o">=</span> <span class="n">feature_list</span>
        <span class="n">input_dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">feature_list</span><span class="p">)</span>
        <span class="c1"># a linear layer without bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">()</span>
    
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># expect the input to be a session of alternatives</span>
        <span class="n">util_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="c1">#!! a trik to prevent the underflow (divide by zero) in the softmax later</span>
        <span class="n">util_values</span> <span class="o">=</span> <span class="n">util_values</span> <span class="o">+</span> <span class="mi">2</span>
        
        <span class="c1"># transpose the result vector before the softmax </span>
        <span class="n">util_values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">t</span><span class="p">(</span><span class="n">util_values</span><span class="p">)</span>
        
        <span class="c1"># convert the softmax values to binary values</span>
        <span class="c1">#max_values, indices = self.softmax(util_values).max()</span>
    
        <span class="c1">#results = np.zeros(len(x))</span>
        <span class="c1">#results[indices] = 1</span>
        <span class="c1">#results = np.transpose(results)</span>
        
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">t</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">util_values</span><span class="p">))</span>

    
    <span class="k">def</span> <span class="nf">l1_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">l1_weight</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">            Return: L1 regularization on all the parameters</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="n">params_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="n">params_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">torch_params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">params_list</span><span class="p">)</span>
            
        <span class="k">return</span> <span class="n">l1_weight</span> <span class="o">*</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">torch_params</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>


    <span class="k">def</span> <span class="nf">l2_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">l2_weight</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">            Return: L2 regularization on all the parameters</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="n">params_list</span> <span class="o">=</span> <span class="p">[]</span>    
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="n">params_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">torch_params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">params_list</span><span class="p">)</span>
            
        <span class="k">return</span> <span class="n">l2_weight</span> <span class="o">*</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">torch_params</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()))</span>


    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">x_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span>
              <span class="n">l1_loss_weight</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>  <span class="c1"># when zero, no L1 regularization</span>
              <span class="n">l2_loss_weight</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
              <span class="n">gpu</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            Train the model with a batch (in our case, also a session) of data</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># expect y_val to be of one_dimension</span>
        <span class="n">y_val</span> <span class="o">=</span> <span class="n">y_val</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_val</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">tensorX</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">x_val</span><span class="p">)</span><span class="o">.</span><span class="n">double</span><span class="p">()</span>
        <span class="n">tensorY</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">y_val</span><span class="p">)</span><span class="o">.</span><span class="n">double</span><span class="p">()</span>

        <span class="k">if</span> <span class="p">(</span><span class="n">gpu</span><span class="p">):</span>
            <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">DoubleTensor</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">DoubleTensor</span>

        <span class="c1"># input variable</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">tensorX</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">dtype</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="c1"># target variable</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">tensorY</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">dtype</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Forward to calculate the losses</span>
        <span class="n">fx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">data_loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">fx</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

        <span class="c1"># optional: add L1 or L2 penalities for regularization</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">l1_loss_weight</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">):</span>
            <span class="n">l1_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1_loss</span><span class="p">(</span><span class="n">l1_loss_weight</span><span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">data_loss</span> <span class="o">+</span> <span class="n">l1_loss</span>

        <span class="k">elif</span> <span class="p">(</span><span class="n">l2_loss_weight</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">):</span>
            <span class="n">l2_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l2_loss</span><span class="p">(</span><span class="n">l2_loss_weight</span><span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">data_loss</span> <span class="o">+</span> <span class="n">l2_loss</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">data_loss</span>

        <span class="c1"># Underflow in the loss calculation</span>
        <span class="k">if</span> <span class="n">math</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;NaN detected&#39;</span><span class="p">)</span>
            <span class="c1">#return output.data[0]</span>

        <span class="k">if</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">LBFGS</span><span class="p">)):</span>
            <span class="k">def</span> <span class="nf">closure</span><span class="p">():</span>
                <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
                <span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">output</span>

            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Reset gradient</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="c1"># Backward</span>
            <span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="c1"># Update parameters</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="c1"># return the cost</span>
        <span class="k">return</span> <span class="n">output</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>


    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_val</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">            Give prediction for alternatives within a single session</span>
<span class="sd">            x_val: DataFrame, or np.ndarray</span>
<span class="sd">            return: numpy</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="n">is_gpu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">is_cuda</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x_val</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">):</span>
            <span class="n">tensorX</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">x_val</span><span class="o">.</span><span class="n">values</span><span class="p">)</span><span class="o">.</span><span class="n">double</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">tensorX</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">x_val</span><span class="p">)</span><span class="o">.</span><span class="n">double</span><span class="p">()</span>
    
        <span class="k">if</span> <span class="p">(</span><span class="n">is_gpu</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">tensorX</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">DoubleTensor</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">tensorX</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
        <span class="k">if</span> <span class="p">(</span><span class="n">is_gpu</span><span class="p">):</span>
            <span class="c1"># get the data from the memory of GPU into CPU</span>
            <span class="n">prob</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">prob</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        
        <span class="k">if</span> <span class="p">(</span><span class="n">binary</span><span class="p">):</span>
            <span class="c1"># convert the softmax values to binary values</span>
            <span class="n">max_indice</span> <span class="o">=</span> <span class="n">prob</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">prob</span><span class="p">))</span>
            <span class="n">ret</span><span class="p">[</span><span class="n">max_indice</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="k">return</span> <span class="n">ret</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">prob</span>


    <span class="k">def</span> <span class="nf">get_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">            Return the Variable of the MNL parameters,</span>
<span class="sd">              which can be updated manually.</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">and</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;linear.weight&#39;</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">param</span>
        <span class="k">return</span> <span class="kc">None</span>

    
    <span class="k">def</span> <span class="nf">print_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">            Print all the parameters within the model</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="k">if</span> <span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">is_cuda</span><span class="p">):</span>
            <span class="n">values</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">values</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        
        <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">feature</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">feature_list</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="n">values</span><span class="p">[</span><span class="n">index</span><span class="p">])</span>
    
    
    <span class="k">def</span> <span class="nf">get_feature_weight</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feature_name</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39; Retrieve the weight of the desired feature &#39;&#39;&#39;</span>
        <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">if</span> <span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">is_cuda</span><span class="p">):</span>
            <span class="n">param_values</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">param_values</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        
        <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">feature</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">feature_list</span><span class="p">):</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">feature_name</span> <span class="o">==</span> <span class="n">feature</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">param_values</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
        
        <span class="c1"># did not find the specified feature</span>
        <span class="k">return</span> <span class="kc">None</span>
    
    
    <span class="k">def</span> <span class="nf">get_feature_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39; get the dictionary of feature weights &#39;&#39;&#39;</span>
        <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">if</span> <span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">is_cuda</span><span class="p">):</span>
            <span class="n">param_values</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">param_values</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        
        <span class="n">feature_weights</span> <span class="o">=</span> <span class="p">{}</span>
        
        <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">feature</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">feature_list</span><span class="p">):</span>
            <span class="n">feature_weights</span><span class="p">[</span><span class="n">feature</span><span class="p">]</span> <span class="o">=</span> <span class="n">param_values</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
        
        <span class="k">return</span> <span class="n">feature_weights</span>
    
    
    <span class="k">def</span> <span class="nf">set_feature_weight</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feature_name</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39; Reset the specified feature weight</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">is_found</span> <span class="o">=</span> <span class="kc">False</span>
        
        <span class="k">try</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">feature</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">feature_list</span><span class="p">):</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">feature_name</span> <span class="o">==</span> <span class="n">feature</span><span class="p">):</span>
                    <span class="n">is_found</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="c1"># override the parameters within the model</span>
                    <span class="n">params</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">except</span> <span class="ne">RuntimeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="c1">#print(&#39;RuntimeError: &#39;, e)</span>
            <span class="c1">#print(&#39;One can ignore this error, since the parameters are still updated !&#39;)</span>
            <span class="k">pass</span>
        
        <span class="k">return</span> <span class="n">is_found</span>


    <span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">file_name</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">            Serialize the model object into a file</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file_name</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">model_file</span><span class="p">:</span>
            <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_file</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;save model to &#39;</span><span class="p">,</span> <span class="n">file_name</span><span class="p">)</span>


    <span class="k">def</span> <span class="nf">set_train_config</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_config</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">            Set the training configs along with the model,</span>
<span class="sd">             so that it can be serialized together with the model.</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_config</span> <span class="o">=</span> <span class="n">train_config</span>


    <span class="k">def</span> <span class="nf">get_train_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_config</span>


<span class="k">def</span> <span class="nf">load_model</span><span class="p">(</span><span class="n">pickle_file</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Instantialize a model from its pickle file.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">pickle_file</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">inp</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># python 3</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;bytes&#39;</span><span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="c1"># python 2</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;load model from &#39;</span><span class="p">,</span> <span class="n">pickle_file</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">model</span>


<span class="k">def</span> <span class="nf">build_model</span><span class="p">(</span><span class="n">input_dim</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Another way to build the model.</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span>
                     <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
    
    <span class="c1"># We need the softmax layer here for the binary cross entropy later </span>
    <span class="n">model</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;softmax&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">())</span>
    
    <span class="k">return</span> <span class="n">model</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">log_loss</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">0.01</span><span class="p">,</span><span class="mf">0.99</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor(4.6052)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

